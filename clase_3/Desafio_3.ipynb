{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gonzalosil/PNL/blob/main/clase_3/Desafio_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8ca6617",
      "metadata": {
        "id": "d8ca6617"
      },
      "source": [
        "### Datos\n",
        "Utilizare como dataset el tercer libro de la saga Harry Potter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "dataset_path = '03 Harry Potter and the Prisoner of Azkaban.txt'\n",
        "if not os.path.exists(dataset_path):\n",
        "\n",
        "  !wget https://raw.githubusercontent.com/gonzalosil/PNL/main/clase_2/HP_books/03%20Harry%20Potter%20and%20the%20Prisoner%20of%20Azkaban.txt\n"
      ],
      "metadata": {
        "id": "J1zp_xjdiWu1",
        "outputId": "b6e96461-f276-4396-b8ae-1d9d411d8a98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "J1zp_xjdiWu1",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-09 13:18:04--  https://raw.githubusercontent.com/gonzalosil/PNL/main/clase_2/HP_books/03%20Harry%20Potter%20and%20the%20Prisoner%20of%20Azkaban.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 653255 (638K) [text/plain]\n",
            "Saving to: ‘03 Harry Potter and the Prisoner of Azkaban.txt’\n",
            "\n",
            "03 Harry Potter and 100%[===================>] 637.94K  --.-KB/s    in 0.08s   \n",
            "\n",
            "2025-04-09 13:18:05 (7.52 MB/s) - ‘03 Harry Potter and the Prisoner of Azkaban.txt’ saved [653255/653255]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "id": "046fc10b",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-07T13:30:03.403723Z",
          "start_time": "2025-04-07T13:30:03.390363Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "046fc10b",
        "outputId": "f28d4f08-53f8-4214-f728-aed744d046f5"
      },
      "source": [
        "with open(dataset_path, 'r') as f:\n",
        "  text = f.read()\n",
        "\n",
        "print(len(text))\n",
        "print(text[:1000])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "625735\n",
            "Harry Potter was a highly unusual boy in many ways. For one thing, he hated the summer holidays more than any other time of year. For another, he really wanted to do his homework, but was forced to do it in secret, in the dead of night. And he also happened to be a wizard.\n",
            "\n",
            "It was nearly midnight, and he was lying on his front in bed, the blankets drawn right over his head like a tent, a torch in one hand and a large leather-bound book (A History of Magic, by Adalbert Waffling) propped open against the pillow. Harry moved the tip of his eagle-feather quill down the page, frowning as he looked for something that would help him write his essay, ‘Witch-Burning in the Fourteenth Century Was Completely Pointless – discuss’.\n",
            "\n",
            "The quill paused at the top of a likely-looking paragraph. Harry pushed his round glasses up his nose, moved his torch closer to the book and read:\n",
            "\n",
            "\n",
            "\n",
            "Non-magic people (more commonly known as Muggles) were particularly afraid of magic in medieval times, but not very goo\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "id": "f9649b79",
      "metadata": {
        "id": "f9649b79"
      },
      "source": [
        "### Elegir el tamaño del contexto\n",
        "Para este ejercicio seleccionaremos un tamaño fijo de contexto de 5 palabras."
      ]
    },
    {
      "cell_type": "code",
      "id": "9997476c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-07T13:30:16.557537Z",
          "start_time": "2025-04-07T13:30:16.533096Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9997476c",
        "outputId": "b22f9656-394c-4169-87de-5f412f770e89"
      },
      "source": [
        "max_context_size = 100\n",
        "\n",
        "# Usaremos las utilidades de procesamiento de textos y secuencias de Keras\n",
        "\n",
        "from tensorflow.keras.utils import pad_sequences # se utilizará para padding\n",
        "\n",
        "# en este caso el vocabulario es el conjunto único de caracteres que existe en tod el texto\n",
        "chars_vocab = set(text)\n",
        "\n",
        "# la longitud de vocabulario de caracteres es:\n",
        "print(len(chars_vocab))\n",
        "\n",
        "# Construimos los dicionarios que asignan índices a caracteres y viceversa.\n",
        "# El diccionario `char2idx` servirá como tokenizador.\n",
        "char2idx = {k: v for v,k in enumerate(chars_vocab)}\n",
        "idx2char = {v: k for k,v in char2idx.items()}\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "id": "06a0b43f",
      "metadata": {
        "id": "06a0b43f"
      },
      "source": [
        "###  Tokenizar"
      ]
    },
    {
      "cell_type": "code",
      "id": "49057573",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-07T13:30:45.460157Z",
          "start_time": "2025-04-07T13:30:45.424805Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49057573",
        "outputId": "4bce084b-c654-46ee-cd9d-5b2037bccc15"
      },
      "source": [
        "# tokenizamos el texto completo\n",
        "tokenized_text = [char2idx[ch] for ch in text]\n",
        "tokenized_text[:25]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[11,\n",
              " 38,\n",
              " 0,\n",
              " 0,\n",
              " 61,\n",
              " 20,\n",
              " 18,\n",
              " 15,\n",
              " 23,\n",
              " 23,\n",
              " 13,\n",
              " 0,\n",
              " 20,\n",
              " 2,\n",
              " 38,\n",
              " 62,\n",
              " 20,\n",
              " 38,\n",
              " 20,\n",
              " 44,\n",
              " 12,\n",
              " 25,\n",
              " 44,\n",
              " 1,\n",
              " 61]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "id": "c8d151c4",
      "metadata": {
        "id": "c8d151c4"
      },
      "source": [
        "### Organizando y estructurando el dataset"
      ]
    },
    {
      "cell_type": "code",
      "id": "c5a3cc80",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-07T13:38:04.943128Z",
          "start_time": "2025-04-07T13:38:00.383659Z"
        },
        "id": "c5a3cc80"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# separaremos el dataset entre entrenamiento y validación.\n",
        "# `p_val` será la proporción del corpus que se reservará para validación\n",
        "# `num_val` es la cantidad de secuencias de tamaño `max_context_size` que se usará en validación\n",
        "p_val = 0.1\n",
        "num_val = int(np.ceil(len(tokenized_text)*p_val/max_context_size))\n",
        "\n",
        "# separamos la porción de texto utilizada en entrenamiento de la de validación.\n",
        "train_text = tokenized_text[:-num_val*max_context_size]\n",
        "val_text = tokenized_text[-num_val*max_context_size:]\n",
        "\n",
        "tokenized_sentences_val = [val_text[init*max_context_size:init*(max_context_size+1)] for init in range(num_val)]\n",
        "\n",
        "tokenized_sentences_train = [train_text[init:init+max_context_size] for init in range(len(train_text)-max_context_size+1)]\n",
        "\n",
        "X = np.array(tokenized_sentences_train[:-1])\n",
        "y = np.array(tokenized_sentences_train[1:])"
      ],
      "outputs": [],
      "execution_count": 5
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-07T13:38:06.549053Z",
          "start_time": "2025-04-07T13:38:06.545083Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ae72b155cf3ab5b",
        "outputId": "10c92e00-8039-4978-e387-c822933f7722"
      },
      "cell_type": "code",
      "source": [
        "print(len(tokenized_sentences_val))\n",
        "print(len(tokenized_sentences_val[0]))"
      ],
      "id": "7ae72b155cf3ab5b",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "626\n",
            "0\n"
          ]
        }
      ],
      "execution_count": 6
    },
    {
      "metadata": {
        "id": "9b44e3b132cc69f4"
      },
      "cell_type": "markdown",
      "source": [
        "Nótese que estamos estructurando el problema de aprendizaje como many-to-many:\n",
        "\n",
        "Entrada: secuencia de tokens $[x_0,x_1, ..., x_n]$\n",
        "\n",
        "Target: secuencia de tokens $[x_1,x_2, ...,x_{n+1}]$\n",
        "\n",
        "De manera que la red tiene que aprender que su salida deben ser los tokens desplazados en una posición y un nuevo token predicho (el N+1).\n",
        "\n",
        "La ventaja de estructurar el aprendizaje de esta manera es que para cada token de target se propaga una señal de gradiente por el grafo de cómputo recurrente, que es mejor que estructurar el problema como many-to-one en donde sólo una señal de gradiente se propaga."
      ],
      "id": "9b44e3b132cc69f4"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-07T13:39:19.873462Z",
          "start_time": "2025-04-07T13:39:19.866004Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb99e1e59ff7f773",
        "outputId": "c33102ca-6421-48b6-a575-fe59488948ac"
      },
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "id": "fb99e1e59ff7f773",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(563035, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-07T13:39:23.000095Z",
          "start_time": "2025-04-07T13:39:22.994276Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dee95952108e16d5",
        "outputId": "7091e9b3-fa77-41ea-f3a6-0a9629356e8d"
      },
      "cell_type": "code",
      "source": [
        "X[0,:10]"
      ],
      "id": "dee95952108e16d5",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([22, 14, 60, 60, 46, 39, 20, 24, 63, 63])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-07T13:39:25.801705Z",
          "start_time": "2025-04-07T13:39:25.794448Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98dfa86e966b6a9b",
        "outputId": "070732a3-8c18-4d1c-9d11-2e60d651759c"
      },
      "cell_type": "code",
      "source": [
        "y[0,:10]"
      ],
      "id": "98dfa86e966b6a9b",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([14, 60, 60, 46, 39, 20, 24, 63, 63, 52])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-07T13:39:26.768240Z",
          "start_time": "2025-04-07T13:39:26.764227Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd49c500ffa1ca23",
        "outputId": "96b81107-f004-4c02-ea88-cf99e516f869"
      },
      "cell_type": "code",
      "source": [
        "vocab_size = len(chars_vocab)\n",
        "vocab_size"
      ],
      "id": "fd49c500ffa1ca23",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "75"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "id": "4675fc07",
      "metadata": {
        "id": "4675fc07"
      },
      "source": [
        "## Definiendo el modelo\n",
        "Usaremos una red neuronal simple con embedding, RNN y una capa final linear."
      ]
    },
    {
      "cell_type": "code",
      "id": "e5de31cd",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-07T13:46:29.365646Z",
          "start_time": "2025-04-07T13:46:29.349650Z"
        },
        "id": "e5de31cd"
      },
      "source": [
        "from keras.layers import Input, TimeDistributed, CategoryEncoding, SimpleRNN, Dense\n",
        "from keras.models import Model, Sequential"
      ],
      "outputs": [],
      "execution_count": 8
    },
    {
      "metadata": {
        "id": "31c9e41324a056d9"
      },
      "cell_type": "markdown",
      "source": [
        "El modelo que se propone como ejemplo consume los índices de los tokens y los transforma en vectores OHE (en este caso no entrenamos una capa de embedding para caracteres). Esa transformación se logra combinando las capas CategoryEncoding que transforma a índices a vectores OHE y TimeDistributed que aplica la capa a lo largo de la dimensión \"temporal\" de la secuencia."
      ],
      "id": "31c9e41324a056d9"
    },
    {
      "cell_type": "code",
      "id": "c7683727",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-07T13:48:38.930224Z",
          "start_time": "2025-04-07T13:48:38.867552Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "c7683727",
        "outputId": "28748636-1a58-42fd-bd43-832f7cf3a3ea"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Input(shape=(None, 1)))\n",
        "model.add(TimeDistributed(CategoryEncoding(num_tokens=vocab_size, output_mode=\"one_hot\")))\n",
        "\n",
        "model.add(SimpleRNN(200, return_sequences=True, dropout=0.1, recurrent_dropout=0.1 ))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop')\n",
        "\n",
        "model.summary()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ time_distributed (\u001b[38;5;33mTimeDistributed\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m75\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)           │          \u001b[38;5;34m55,200\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m75\u001b[0m)            │          \u001b[38;5;34m15,075\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ time_distributed (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">55,200</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">15,075</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m70,275\u001b[0m (274.51 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">70,275</span> (274.51 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m70,275\u001b[0m (274.51 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">70,275</span> (274.51 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definir modelo"
      ],
      "metadata": {
        "id": "JY-RV3kOlBLq"
      },
      "id": "JY-RV3kOlBLq"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "class PplCallback(Callback):\n",
        "\n",
        "    '''\n",
        "    Este callback es una solución ad-hoc para calcular al final de cada epoch de\n",
        "    entrenamiento la métrica de Perplejidad sobre un conjunto de datos de validación.\n",
        "    La perplejidad es una métrica cuantitativa para evaluar la calidad de la generación de secuencias.\n",
        "    Además implementa la finalización del entrenamiento (Early Stopping)\n",
        "    si la perplejidad no mejora después de `patience` epochs.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, val_data, history_ppl, max_context_size, batch_size=512, patience=5):\n",
        "        # El callback lo inicializamos con secuencias de validación sobre las cuales\n",
        "        # mediremos la perplejidad\n",
        "        self.val_data = val_data\n",
        "        self.history_ppl = history_ppl\n",
        "        self.max_context_size = max_context_size\n",
        "        self.batch_size = batch_size\n",
        "        self.patience = patience\n",
        "\n",
        "        self.min_score = np.inf\n",
        "        self.patience_counter = 0\n",
        "\n",
        "        self.X = []\n",
        "        self.y = []\n",
        "\n",
        "        # nos movemos en todas las secuencias de los datos de validación\n",
        "        for seq in self.val_data:\n",
        "            len_seq = len(seq)\n",
        "\n",
        "            # armamos todas las subsecuencias\n",
        "            for i in range(1, len_seq):\n",
        "                subseq = seq[:i]\n",
        "                self.X.append(subseq)\n",
        "                self.y.append(seq[i])\n",
        "\n",
        "        # rellenamos todas las subsecuencias a la izquierda (padding)\n",
        "        self.X = pad_sequences(self.X, maxlen=self.max_context_size, padding='pre')\n",
        "        self.y = np.array(self.y)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "\n",
        "        # en `scores` iremos guardando la perplejidad promedio por batch\n",
        "        scores = []\n",
        "\n",
        "        # predecimos por lotes para evitar problemas de memoria\n",
        "        for i in range(0, len(self.X), self.batch_size):\n",
        "            x_batch = self.X[i:i+self.batch_size]\n",
        "            y_batch = self.y[i:i+self.batch_size]\n",
        "\n",
        "            predictions = self.model.predict(x_batch, verbose=0)\n",
        "\n",
        "            # en `probs` iremos guardando las probabilidades de los términos target\n",
        "            probs = predictions[np.arange(len(y_batch)), -1, y_batch]\n",
        "\n",
        "            # calculamos la perplejidad por medio de logaritmos\n",
        "            log_probs = np.log(probs + 1e-10)  # evitamos log(0)\n",
        "            perplexity = np.exp(-np.mean(log_probs))\n",
        "\n",
        "            scores.append(perplexity)\n",
        "\n",
        "        # promediamos todos los scores e imprimimos el valor promedio\n",
        "        current_score = np.mean(scores)\n",
        "        self.history_ppl.append(current_score)\n",
        "        print(f'\\n mean perplexity: {current_score} \\n')\n",
        "\n",
        "        # chequeamos si tenemos que detener el entrenamiento\n",
        "        if current_score < self.min_score:\n",
        "            self.min_score = current_score\n",
        "            self.model.save(\"my_model.keras\")\n",
        "            print(\"Saved new model!\")\n",
        "            self.patience_counter = 0\n",
        "        else:\n",
        "            self.patience_counter += 1\n",
        "            if self.patience_counter == self.patience:\n",
        "                print(\"Stopping training...\")\n",
        "                self.model.stop_training = True\n"
      ],
      "metadata": {
        "id": "Fevn6oO-r6u1"
      },
      "id": "Fevn6oO-r6u1",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para calcular la perplejidad sobre el conjunto de validación, se implementó un callback personalizado. La primera versión realizaba predicciones sobre todo el conjunto de validación a la vez, lo que generaba un alto consumo de memoria. Para solucionar esto, se optimizó el callback dividiendo el conjunto de validación en lotes (batch_size) y procesando cada lote por separado, reduciendo significativamente el uso de memoria, ya que previamente se llenaba la memoria de colab"
      ],
      "metadata": {
        "id": "-Iv5bzGst1sU"
      },
      "id": "-Iv5bzGst1sU"
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow import keras\n",
        "\n",
        "# class PplCallback(keras.callbacks.Callback):\n",
        "\n",
        "#     '''\n",
        "#     Este callback es una solución ad-hoc para calcular al final de cada epoch de\n",
        "#     entrenamiento la métrica de Perplejidad sobre un conjunto de datos de validación.\n",
        "#     La perplejidad es una métrica cuantitativa para evaluar la calidad de la generación de secuencias.\n",
        "#     Además implementa la finalización del entrenamiento (Early Stopping)\n",
        "#     si la perplejidad no mejora después de `patience` epochs.\n",
        "#     '''\n",
        "\n",
        "#     def __init__(self, val_data, history_ppl,patience=5):\n",
        "#       # El callback lo inicializamos con secuencias de validación sobre las cuales\n",
        "#       # mediremos la perplejidad\n",
        "#       self.val_data = val_data\n",
        "\n",
        "#       self.target = []\n",
        "#       self.padded = []\n",
        "\n",
        "#       count = 0\n",
        "#       self.info = []\n",
        "#       self.min_score = np.inf\n",
        "#       self.patience_counter = 0\n",
        "#       self.patience = patience\n",
        "\n",
        "#       # nos movemos en todas las secuencias de los datos de validación\n",
        "#       for seq in self.val_data:\n",
        "\n",
        "#         len_seq = len(seq)\n",
        "#         # armamos todas las subsecuencias\n",
        "#         subseq = [seq[:i] for i in range(1,len_seq)]\n",
        "#         self.target.extend([seq[i] for i in range(1,len_seq)])\n",
        "\n",
        "#         if len(subseq)!=0:\n",
        "\n",
        "#           self.padded.append(pad_sequences(subseq, maxlen=max_context_size, padding='pre'))\n",
        "\n",
        "#           self.info.append((count,count+len_seq))\n",
        "#           count += len_seq\n",
        "\n",
        "#       self.padded = np.vstack(self.padded)\n",
        "\n",
        "\n",
        "#     def on_epoch_end(self, epoch, logs=None):\n",
        "\n",
        "#         # en `scores` iremos guardando la perplejidad de cada secuencia\n",
        "#         scores = []\n",
        "\n",
        "#         predictions = self.model.predict(self.padded,verbose=0)\n",
        "\n",
        "#         # para cada secuencia de validación\n",
        "#         for start,end in self.info:\n",
        "\n",
        "#           # en `probs` iremos guardando las probabilidades de los términos target\n",
        "#           probs = [predictions[idx_seq,-1,idx_vocab] for idx_seq, idx_vocab in zip(range(start,end),self.target[start:end])]\n",
        "\n",
        "#           # calculamos la perplejidad por medio de logaritmos\n",
        "#           scores.append(np.exp(-np.sum(np.log(probs))/(end-start)))\n",
        "\n",
        "#         # promediamos todos los scores e imprimimos el valor promedio\n",
        "#         current_score = np.mean(scores)\n",
        "#         history_ppl.append(current_score)\n",
        "#         print(f'\\n mean perplexity: {current_score} \\n')\n",
        "\n",
        "#         # chequeamos si tenemos que detener el entrenamiento\n",
        "#         if current_score < self.min_score:\n",
        "#           self.min_score = current_score\n",
        "#           self.model.save(\"my_model.keras\")\n",
        "#           print(\"Saved new model!\")\n",
        "#           self.patience_counter = 0\n",
        "#         else:\n",
        "#           self.patience_counter += 1\n",
        "#           if self.patience_counter == self.patience:\n",
        "#             print(\"Stopping training...\")\n",
        "#             self.model.stop_training = True"
      ],
      "metadata": {
        "id": "8JJkt3lAk74y"
      },
      "id": "8JJkt3lAk74y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2c0ba6ec",
      "metadata": {
        "id": "2c0ba6ec"
      },
      "source": [
        "### Entrenamiento\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22378cad",
      "metadata": {
        "id": "22378cad"
      },
      "source": [
        "### Definir el modelo"
      ]
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b669b40864d8714d",
        "outputId": "480f0f64-8648-46de-a9f9-6824a1792f43"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m2200/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.3902\n",
            " mean perplexity: 5.233357906341553 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m2200/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 48ms/step - loss: 2.3901\n",
            "Epoch 2/20\n",
            "\u001b[1m2200/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.8354\n",
            " mean perplexity: 4.571587562561035 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m2200/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 43ms/step - loss: 1.8354\n",
            "Epoch 3/20\n",
            "\u001b[1m2198/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.7455\n",
            " mean perplexity: 4.334033966064453 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m2200/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 44ms/step - loss: 1.7455\n",
            "Epoch 4/20\n",
            "\u001b[1m2197/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.7078\n",
            " mean perplexity: 4.263725757598877 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m2200/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 43ms/step - loss: 1.7078\n",
            "Epoch 5/20\n",
            "\u001b[1m2199/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.6858\n",
            " mean perplexity: 4.137736797332764 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m2200/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 43ms/step - loss: 1.6858\n",
            "Epoch 6/20\n",
            "\u001b[1m2198/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.6710\n",
            " mean perplexity: 4.08929443359375 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m2200/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 44ms/step - loss: 1.6709\n",
            "Epoch 7/20\n",
            "\u001b[1m2197/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.6612\n",
            " mean perplexity: 4.053728103637695 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m2200/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 43ms/step - loss: 1.6612\n",
            "Epoch 8/20\n",
            "\u001b[1m2197/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.6521\n",
            " mean perplexity: 4.011045932769775 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m2200/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 44ms/step - loss: 1.6521\n",
            "Epoch 9/20\n",
            "\u001b[1m2198/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.6454\n",
            " mean perplexity: 3.993011236190796 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m2200/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 44ms/step - loss: 1.6454\n",
            "Epoch 10/20\n",
            "\u001b[1m2197/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.6397\n",
            " mean perplexity: 3.9489574432373047 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m2200/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 44ms/step - loss: 1.6397\n",
            "Epoch 11/20\n",
            "\u001b[1m2200/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.6342\n",
            " mean perplexity: 3.9504287242889404 \n",
            "\n",
            "\u001b[1m2200/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 44ms/step - loss: 1.6342\n",
            "Epoch 12/20\n",
            "\u001b[1m2198/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.6293\n",
            " mean perplexity: 3.9395241737365723 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m2200/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 43ms/step - loss: 1.6293\n",
            "Epoch 13/20\n",
            "\u001b[1m2198/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.6267\n",
            " mean perplexity: 3.900864601135254 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m2200/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 43ms/step - loss: 1.6267\n",
            "Epoch 14/20\n",
            "\u001b[1m2198/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.6237\n",
            " mean perplexity: 3.901409149169922 \n",
            "\n",
            "\u001b[1m2200/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 44ms/step - loss: 1.6237\n",
            "Epoch 15/20\n",
            "\u001b[1m2199/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.6210\n",
            " mean perplexity: 3.9030613899230957 \n",
            "\n",
            "\u001b[1m2200/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 44ms/step - loss: 1.6210\n",
            "Epoch 16/20\n",
            "\u001b[1m2197/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.6184\n",
            " mean perplexity: 3.891587972640991 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m2200/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 44ms/step - loss: 1.6184\n",
            "Epoch 17/20\n",
            "\u001b[1m2200/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.6164\n",
            " mean perplexity: 3.8861372470855713 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m2200/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 44ms/step - loss: 1.6164\n",
            "Epoch 18/20\n",
            "\u001b[1m2199/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.6138\n",
            " mean perplexity: 3.862208843231201 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m2200/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 44ms/step - loss: 1.6138\n",
            "Epoch 19/20\n",
            "\u001b[1m2197/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.6126\n",
            " mean perplexity: 3.8631644248962402 \n",
            "\n",
            "\u001b[1m2200/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 44ms/step - loss: 1.6126\n",
            "Epoch 20/20\n",
            "\u001b[1m2198/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.6110\n",
            " mean perplexity: 3.890751838684082 \n",
            "\n",
            "\u001b[1m2200/2200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 45ms/step - loss: 1.6110\n"
          ]
        }
      ],
      "execution_count": null,
      "source": [
        "# fiteamos, nótese el agregado del callback con su inicialización. El batch_size lo podemos seleccionar a mano\n",
        "# en general, lo mejor es escoger el batch más grande posible que minimice el tiempo de cada época.\n",
        "# En la variable `history_ppl` se guardarán los valores de perplejidad para cada época.\n",
        "history_ppl = []\n",
        "hist = model.fit(X, y, epochs=20, callbacks=[PplCallback(tokenized_sentences_val, history_ppl, max_context_size=max_context_size)], batch_size=256)"
      ],
      "id": "b669b40864d8714d"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "8a338e33f0a5c4ab",
        "outputId": "2e2f28bb-8fe3-4837-a43c-ee362850f96c"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Entrenamiento\n",
        "epoch_count = range(1, len(history_ppl) + 1)\n",
        "sns.lineplot(x=epoch_count,  y=history_ppl)\n",
        "plt.show()"
      ],
      "id": "8a338e33f0a5c4ab"
    },
    {
      "metadata": {
        "id": "1f8dd1dd05cf1c27"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "from tensorflow import keras\n",
        "# Cargamos el mejor modelo guardado del entrenamiento para hacer inferencia\n",
        "model = keras.models.load_model('my_model.keras')"
      ],
      "id": "1f8dd1dd05cf1c27"
    },
    {
      "metadata": {
        "id": "d6f66ecb9c75984c"
      },
      "cell_type": "markdown",
      "source": [
        "### Prediccion del proximo caracter"
      ],
      "id": "d6f66ecb9c75984c"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c1faa7f8414ef7c",
        "outputId": "201f10dd-a58e-4a8c-fd79-1d0aacb88143"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m107.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "execution_count": null,
      "source": [
        "# Se puede usar gradio para probar el modelo\n",
        "# Gradio es una herramienta muy útil para crear interfaces para ensayar modelos\n",
        "# https://gradio.app/\n",
        "\n",
        "!pip install -q gradio"
      ],
      "id": "2c1faa7f8414ef7c"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "id": "3f2a9152bebe77b3",
        "outputId": "9199b3e0-a006-4657-dd0a-13c615f001f1"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://e3727ac4c5ae666303.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e3727ac4c5ae666303.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "Created dataset file at: .gradio/flagged/dataset1.csv\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://e3727ac4c5ae666303.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "execution_count": null,
      "source": [
        "import gradio as gr\n",
        "\n",
        "def model_response(human_text):\n",
        "\n",
        "    # Encodeamos\n",
        "    encoded = [char2idx[ch] for ch in human_text.lower() ]\n",
        "    # Si tienen distinto largo\n",
        "    encoded = pad_sequences([encoded], maxlen=max_context_size, padding='pre')\n",
        "\n",
        "    # Predicción softmax\n",
        "    y_hat = np.argmax(model.predict(encoded)[0,-1,:])\n",
        "\n",
        "\n",
        "    # Debemos buscar en el vocabulario el caracter\n",
        "    # que corresopnde al indice (y_hat) predicho por le modelo\n",
        "    out_word = ''\n",
        "    out_word = idx2char[y_hat]\n",
        "\n",
        "    # Agrego la palabra a la frase predicha\n",
        "    return human_text + out_word\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=model_response,\n",
        "    inputs=[\"textbox\"],\n",
        "    outputs=\"text\")\n",
        "\n",
        "iface.launch(debug=True)"
      ],
      "id": "3f2a9152bebe77b3"
    },
    {
      "metadata": {
        "id": "c9a63c55346ddbc1"
      },
      "cell_type": "markdown",
      "source": [
        "### Generacion de secuencias"
      ],
      "id": "c9a63c55346ddbc1"
    },
    {
      "metadata": {
        "id": "4879a0cb20b18369"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "def generate_seq(model, seed_text, max_length, n_words):\n",
        "    \"\"\"\n",
        "        Exec model sequence prediction\n",
        "\n",
        "        Args:\n",
        "            model (keras): modelo entrenado\n",
        "            seed_text (string): texto de entrada (input_seq)\n",
        "            max_length (int): máxima longitud de la sequencia de entrada\n",
        "            n_words (int): números de caracteres a agregar a la sequencia de entrada\n",
        "        returns:\n",
        "            output_text (string): sentencia con las \"n_words\" agregadas\n",
        "    \"\"\"\n",
        "    output_text = seed_text\n",
        "\t# generate a fixed number of words\n",
        "    for _ in range(n_words):\n",
        "\t\t# Encodeamos\n",
        "        encoded = [char2idx[ch] for ch in output_text.lower() ]\n",
        "\t\t# Si tienen distinto largo\n",
        "        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
        "\n",
        "\t\t# Predicción softmax\n",
        "        y_hat = np.argmax(model.predict(encoded,verbose=0)[0,-1,:])\n",
        "\t\t# Vamos concatenando las predicciones\n",
        "        out_word = ''\n",
        "\n",
        "        out_word = idx2char[y_hat]\n",
        "\n",
        "\t\t# Agrego las palabras a la frase predicha\n",
        "        output_text += out_word\n",
        "    return output_text\n"
      ],
      "id": "4879a0cb20b18369"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "d237c87428b5b7d1",
        "outputId": "6f92a44e-cbcd-4ed3-a5fd-47e7794159e1"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hermione was started to t'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "execution_count": null,
      "source": [
        "input_text='Hermi'\n",
        "\n",
        "generate_seq(model, input_text, max_length=max_context_size, n_words=20)"
      ],
      "id": "d237c87428b5b7d1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Beam search y muestreo aleatorio\n"
      ],
      "metadata": {
        "id": "XWCmWLfW5DFy"
      },
      "id": "XWCmWLfW5DFy"
    },
    {
      "cell_type": "code",
      "source": [
        "# funcionalidades para hacer encoding y decoding\n",
        "\n",
        "def encode(text,max_length=max_context_size):\n",
        "\n",
        "    encoded = [char2idx[ch] for ch in text]\n",
        "    encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
        "\n",
        "    return encoded\n",
        "\n",
        "def decode(seq):\n",
        "    return ''.join([idx2char[ch] for ch in seq])"
      ],
      "metadata": {
        "id": "6MXGc-Y95Fg9"
      },
      "id": "6MXGc-Y95Fg9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.special import softmax\n",
        "\n",
        "# función que selecciona candidatos para el beam search\n",
        "def select_candidates(pred,num_beams,vocab_size,history_probs,history_tokens,temp,mode):\n",
        "\n",
        "  # colectar todas las probabilidades para la siguiente búsqueda\n",
        "  pred_large = []\n",
        "\n",
        "  for idx,pp in enumerate(pred):\n",
        "    pred_large.extend(np.log(pp+1E-10)+history_probs[idx])\n",
        "\n",
        "  pred_large = np.array(pred_large)\n",
        "\n",
        "  # criterio de selección\n",
        "  if mode == 'det':\n",
        "    idx_select = np.argsort(pred_large)[::-1][:num_beams] # beam search determinista\n",
        "  elif mode == 'sto':\n",
        "    idx_select = np.random.choice(np.arange(pred_large.shape[0]), num_beams, p=softmax(pred_large/temp)) # beam search con muestreo aleatorio\n",
        "  else:\n",
        "    raise ValueError(f'Wrong selection mode. {mode} was given. det and sto are supported.')\n",
        "\n",
        "  # traducir a índices de token en el vocabulario\n",
        "  new_history_tokens = np.concatenate((np.array(history_tokens)[idx_select//vocab_size],\n",
        "                        np.array([idx_select%vocab_size]).T),\n",
        "                      axis=1)\n",
        "\n",
        "  # devolver el producto de las probabilidades (log) y la secuencia de tokens seleccionados\n",
        "  return pred_large[idx_select.astype(int)], new_history_tokens.astype(int)\n",
        "\n",
        "\n",
        "def beam_search(model,num_beams,num_words,input,temp=1,mode='det'):\n",
        "\n",
        "    # first iteration\n",
        "\n",
        "    # encode\n",
        "    encoded = encode(input)\n",
        "\n",
        "    # first prediction\n",
        "    y_hat = model.predict(encoded,verbose=0)[0,-1,:]\n",
        "\n",
        "    # get vocabulary size\n",
        "    vocab_size = y_hat.shape[0]\n",
        "\n",
        "    # initialize history\n",
        "    history_probs = [0]*num_beams\n",
        "    history_tokens = [encoded[0]]*num_beams\n",
        "\n",
        "    # select num_beams candidates\n",
        "    history_probs, history_tokens = select_candidates([y_hat],\n",
        "                                        num_beams,\n",
        "                                        vocab_size,\n",
        "                                        history_probs,\n",
        "                                        history_tokens,\n",
        "                                        temp,\n",
        "                                        mode)\n",
        "\n",
        "    # beam search loop\n",
        "    for i in range(num_words-1):\n",
        "\n",
        "      preds = []\n",
        "\n",
        "      for hist in history_tokens:\n",
        "\n",
        "        # actualizar secuencia de tokens\n",
        "        input_update = np.array([hist[i+1:]]).copy()\n",
        "\n",
        "        # predicción\n",
        "        y_hat = model.predict(input_update,verbose=0)[0,-1,:]\n",
        "\n",
        "        preds.append(y_hat)\n",
        "\n",
        "      history_probs, history_tokens = select_candidates(preds,\n",
        "                                                        num_beams,\n",
        "                                                        vocab_size,\n",
        "                                                        history_probs,\n",
        "                                                        history_tokens,\n",
        "                                                        temp,\n",
        "                                                        mode)\n",
        "\n",
        "    return history_tokens[:,-(len(input)+num_words):]"
      ],
      "metadata": {
        "id": "fsp8duev5H7J"
      },
      "id": "fsp8duev5H7J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predicción con beam search\n",
        "salidas = beam_search(model,num_beams=10,num_words=35,input=\"Harry Potter\")"
      ],
      "metadata": {
        "id": "02nBUn7Q5Li5"
      },
      "id": "02nBUn7Q5Li5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "salidas[0]"
      ],
      "metadata": {
        "id": "ojblSuoR5PIW",
        "outputId": "25100d7e-45ae-40ff-dfad-c4978d37eccd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ojblSuoR5PIW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 8, 35, 68, 68, 30, 63, 69,  9, 62, 62, 40, 68, 11, 28, 63, 73, 35,\n",
              "       41,  6, 63, 69, 68,  9, 45, 40, 73, 73,  9, 68, 63, 15, 64, 38,  9,\n",
              "       14, 35,  4, 35, 53, 53, 11, 63, 35, 14,  6, 63, 62])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# veamos las salidas\n",
        "decode(salidas[0])"
      ],
      "metadata": {
        "id": "iwvQ20Mr5PdL",
        "outputId": "9019d4b4-d99f-4728-dfb1-cd0d6b5c6add",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "id": "iwvQ20Mr5PdL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Harry Potter,’ said Professor McGonagall, and t'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparación de arquitecturas recurrentes: SimpleRNN vs LSTM vs GRU"
      ],
      "metadata": {
        "id": "E_TbRltnuCAy"
      },
      "id": "E_TbRltnuCAy"
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Input, TimeDistributed, CategoryEncoding, SimpleRNN, LSTM, GRU, Dense\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Configuración de hiperparámetros equivalentes\n",
        "model_configs = {\n",
        "    \"SimpleRNN\": {\"layer\": SimpleRNN, \"params\": {\"units\": 200, \"recurrent_dropout\": 0.1}},\n",
        "    \"LSTM\": {\"layer\": LSTM, \"params\": {\"units\": 92, \"recurrent_dropout\": 0.0}},\n",
        "    \"GRU\": {\"layer\": GRU, \"params\": {\"units\": 109, \"recurrent_dropout\": 0.0}},\n",
        "}\n",
        "\n",
        "history_all = {}\n",
        "history_all_loss = {}\n",
        "# Entrenamiento de los tres modelos\n",
        "for name, config in model_configs.items():\n",
        "    print(f\"\\nEntrenando modelo: {name}\")\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(None, 1)))\n",
        "    model.add(TimeDistributed(CategoryEncoding(num_tokens=vocab_size, output_mode=\"one_hot\")))\n",
        "    model.add(config[\"layer\"](**config[\"params\"], return_sequences=True, dropout=0.1))\n",
        "    model.add(Dense(vocab_size, activation=\"softmax\"))\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\")\n",
        "\n",
        "    history_ppl = []\n",
        "    ppl_callback = PplCallback(\n",
        "        val_data=tokenized_sentences_val,\n",
        "        history_ppl=history_ppl,\n",
        "        max_context_size=max_context_size\n",
        "    )\n",
        "\n",
        "    model.fit(X, y, epochs=10, batch_size=256, callbacks=[ppl_callback], verbose=1)\n",
        "\n",
        "    history_all[name] = history_ppl\n",
        "    history_all_loss[name] = hist.history[\"loss\"]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRppylTxuOqY",
        "outputId": "73fe8739-e80b-4d83-c33f-7041df360d31"
      },
      "id": "JRppylTxuOqY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Entrenando modelo: SimpleRNN\n",
            "Epoch 1/10\n",
            "\u001b[1m 544/2200\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8:58\u001b[0m 325ms/step - loss: 2.7838"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === GRAFICAR PERPLEJIDAD ===\n",
        "plt.figure(figsize=(12, 5))\n",
        "for name, ppl_values in history_all_ppl.items():\n",
        "    plt.plot(ppl_values, label=f\"{name} - Perplejidad\", marker='o')\n",
        "\n",
        "plt.title(\"📈 Perplejidad vs Épocas\")\n",
        "plt.xlabel(\"Época\")\n",
        "plt.ylabel(\"Perplejidad\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# === GRAFICAR TRAINING LOSS ===\n",
        "plt.figure(figsize=(12, 5))\n",
        "for name, loss_values in history_all_loss.items():\n",
        "    plt.plot(loss_values, label=f\"{name} - Training Loss\", marker='x')\n",
        "\n",
        "plt.title(\"📉 Pérdida de entrenamiento vs Épocas\")\n",
        "plt.xlabel(\"Época\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-3RMetcvtRQR"
      },
      "id": "-3RMetcvtRQR",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}